{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNWBnMHRjLCQvhnfaQgxc6S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/gpt2-ft-pipeline/blob/main/notebooks/saved_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p216TognFKVv",
        "outputId": "ee74d3b3-d09a-462e-f9d4-5436fa6ad2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.7/527.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q keras-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras_nlp\n",
        "from tensorflow.python.saved_model import tag_constants"
      ],
      "metadata": {
        "id": "1_Zp2of1FPCH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_tokenizer = keras_nlp.models.GPT2Tokenizer.from_preset(\"gpt2_base_en\")\n",
        "gpt2_preprocessor = keras_nlp.models.GPT2CausalLMPreprocessor.from_preset(\n",
        "    \"gpt2_base_en\",\n",
        "    sequence_length=256,\n",
        "    add_end_token=True,\n",
        ")\n",
        "gpt2_lm = keras_nlp.models.GPT2CausalLM.from_preset(\"gpt2_base_en\", preprocessor=gpt2_preprocessor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyL7kLVoFZHu",
        "outputId": "c1549af5-39af-4ad5-c716-e26e5c9c41e0"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.linalg.matmul_2), but are not present in its tracked objects:   <tf.Variable 'token_embedding/embeddings:0' shape=(50257, 768) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "signature_dict = {\n",
        "    \"prompt\": tf.TensorSpec(shape=[], dtype=tf.string, name=\"prompt\"),\n",
        "    \"max_length\": tf.TensorSpec(shape=[], dtype=tf.int64, name=\"max_length\"),\n",
        "}\n",
        "\n",
        "def gpt2_lm_exporter(model):\n",
        "  @tf.function(input_signature=[signature_dict])\n",
        "  def serving_fn(inputs):\n",
        "    prompt = tf.convert_to_tensor(inputs[\"prompt\"])\n",
        "    input_is_scalar = prompt.shape.rank == 0\n",
        "    prompt = prompt[tf.newaxis] if input_is_scalar else prompt\n",
        "    prompt = model.preprocessor.tokenizer(prompt)\n",
        "\n",
        "    # Pad ragged to dense tensors.\n",
        "    padded_shape = (1, inputs[\"max_length\"])\n",
        "    min_length = tf.reduce_min(prompt.row_lengths())\n",
        "    input_mask = tf.ones_like(prompt, tf.bool).to_tensor(shape=padded_shape)\n",
        "    prompt = prompt.to_tensor(shape=padded_shape)\n",
        "    prompt = tf.cast(prompt, dtype=\"int64\")\n",
        "\n",
        "    generate_function = model.make_generate_function()\n",
        "    output = generate_function({\"token_ids\": prompt, \"padding_mask\": input_mask}, min_length)\n",
        "\n",
        "    token_ids, padding_mask = output[\"token_ids\"], output[\"padding_mask\"]\n",
        "    padding_mask = padding_mask & (token_ids != model.preprocessor.tokenizer.end_token_id)\n",
        "    token_ids = tf.ragged.boolean_mask(token_ids, padding_mask)\n",
        "\n",
        "    token_ids = tf.cast(token_ids, dtype=\"int32\")\n",
        "    unicode_text = tf.strings.reduce_join(\n",
        "        model.preprocessor.tokenizer.id_to_token_map.lookup(token_ids), axis=-1\n",
        "    )\n",
        "    split_unicode_text = tf.strings.unicode_split(unicode_text, \"UTF-8\")\n",
        "    byte_text = tf.strings.reduce_join(\n",
        "        model.preprocessor.tokenizer.unicode2byte.lookup(split_unicode_text), axis=-1\n",
        "    )\n",
        "    byte_text = tf.concat(byte_text, axis=0)\n",
        "    byte_text = tf.squeeze(byte_text, 0)\n",
        "    return {\"result\": byte_text}\n",
        "\n",
        "  return serving_fn"
      ],
      "metadata": {
        "id": "cvS4ywztWEad"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(\n",
        "    gpt2_lm,\n",
        "    \"./gpt_lm_custom/1/\",\n",
        "    signatures={\"serving_default\": gpt2_lm_exporter(gpt2_lm)},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zn9zaMeJ27i",
        "outputId": "199adab3-e06c-40fd-c387-04006cd93de7"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras_nlp.src.models.gpt2.gpt2_causal_lm_preprocessor.GPT2CausalLMPreprocessor object at 0x7f2f05ff06a0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras_nlp.src.layers.start_end_packer.StartEndPacker object at 0x7f2f05ff37f0>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla, gpt2_tokenizer_5_layer_call_fn, gpt2_tokenizer_5_layer_call_and_return_conditional_losses, cached_multi_head_attention_layer_call_fn, cached_multi_head_attention_layer_call_and_return_conditional_losses while saving (showing 5 of 315). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!saved_model_cli show --dir gpt_lm_custom/1/ --tag_set serve --signature_def serving_default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHWq-NEpnvsA",
        "outputId": "d5c2f9d3-3aea-4e76-c512-50884cc51b05"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-16 06:01:39.635389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['max_length'] tensor_info:\n",
            "      dtype: DT_INT64\n",
            "      shape: ()\n",
            "      name: serving_default_max_length:0\n",
            "  inputs['prompt'] tensor_info:\n",
            "      dtype: DT_STRING\n",
            "      shape: ()\n",
            "      name: serving_default_prompt:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['result'] tensor_info:\n",
            "      dtype: DT_STRING\n",
            "      shape: ()\n",
            "      name: StatefulPartitionedCall:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_loaded = tf.saved_model.load(\"gpt_lm_custom/1\", tags=[tag_constants.SERVING])\n",
        "gpt_lm_predict_fn = saved_model_loaded.signatures[\"serving_default\"]"
      ],
      "metadata": {
        "id": "JFWt7OFtXEzJ"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_lm_predict_fn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmHEO0aYobWf",
        "outputId": "8193d6dc-a838-4401-aff4-5312523e920f"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ConcreteFunction signature_wrapper(*, max_length, prompt) at 0x7F2DC74F7E20>"
            ]
          },
          "metadata": {},
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = tf.constant(\"hello world\")\n",
        "max_length = tf.constant(100, dtype=\"int64\")"
      ],
      "metadata": {
        "id": "J4g_hMgolvQY"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = gpt_lm_predict_fn(\n",
        "    prompt=prompt,\n",
        "    max_length=max_length,\n",
        "    # batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "id": "jwv5zIRzoXOZ"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwXiJ_EYpDJE",
        "outputId": "0e191173-890b-477f-83b1-399c85e765e2"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'result': <tf.Tensor: shape=(), dtype=string, numpy=b'hello world of gaming and entertainment.\\n\\nThe game is based on a game called \"Dread Pirate Roberts\" by the creators of the popular game \"Dread Pirate Roberts\" (also known as DOTA or DOTA 2).\\n\\nIt\\'s the second most popular online role-playing game in the world and the second most downloaded game.\\n\\nIn addition to being the most downloaded game on Steam and the most used app, DOTA is the third most downloaded game on the Android App'>}"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OkhBdKlZpUXl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}