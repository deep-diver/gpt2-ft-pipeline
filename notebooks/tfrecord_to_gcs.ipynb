{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPKao8S8oZGajiqjFF3Qae",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deep-diver/gpt2-ft-pipeline/blob/main/notebooks/tfrecord_to_gcs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYqEv4srVXMy",
        "outputId": "60c3d364-3b0f-4fdb-88f5-ba2098145538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt2-ft-pipeline'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 71 (delta 12), reused 48 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (71/71), 61.13 KiB | 1.91 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/deep-diver/gpt2-ft-pipeline.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd gpt2-ft-pipeline/alpaca && tfds build --register_checksums"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnuHUpAEVpti",
        "outputId": "e160df23-32e8-40b1-ee40-c1af8d4ef5bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO[build.py]: Loading dataset  from path: /content/gpt2-ft-pipeline/alpaca/alpaca_dataset_builder.py\n",
            "2023-06-19 06:28:39.036691: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-06-19 06:28:39.873876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-06-19 06:28:40.908831: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"NOT_FOUND: Error executing an HTTP request: HTTP response code 404\".\n",
            "INFO[build.py]: download_and_prepare for dataset alpaca/1.0.0...\n",
            "INFO[dataset_builder.py]: Generating dataset alpaca (/root/tensorflow_datasets/alpaca/1.0.0)\n",
            "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /root/tensorflow_datasets/alpaca/1.0.0...\u001b[0m\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "                                       \n",
            "\u001b[AINFO[download_manager.py]: Downloading https://github.com/tloen/alpaca-lora/raw/main/alpaca_data.json into /root/tensorflow_datasets/downloads/tloen_alpaca-lora_raw_main_alpaca_data4Au3mCuTvwNKcdqSKmlcWQaDUk2-e5GtRsgF8LMbqRE.json.tmp.6a6dd5965f1148ebb642bac80419e189...\n",
            "Dl Completed...: 0 url [00:00, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:00<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:01<?, ? url/s]\n",
            "Dl Size...:   0% 0/6 [00:01<?, ? MiB/s]\u001b[A\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Size...: 13 MiB [00:02,  2.03s/ MiB]\u001b[A\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...:   0% 0/1 [00:02<?, ? url/s]\n",
            "Dl Completed...: 100% 1/1 [00:02<00:00,  2.19s/ url]\n",
            "Dl Size...: 21 MiB [00:02,  9.59 MiB/s]\n",
            "Dl Completed...: 100% 1/1 [00:02<00:00,  2.19s/ url]\n",
            "Generating splits...:   0% 0/2 [00:00<?, ? splits/s]\n",
            "Generating train examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
            "Generating train examples...: 1413 examples [00:00, 14123.41 examples/s]\u001b[A\n",
            "Generating train examples...: 2862 examples [00:00, 14334.72 examples/s]\u001b[A\n",
            "Generating train examples...: 4303 examples [00:00, 14367.62 examples/s]\u001b[A\n",
            "Generating train examples...: 5761 examples [00:00, 14447.55 examples/s]\u001b[A\n",
            "Generating train examples...: 7206 examples [00:00, 13986.49 examples/s]\u001b[A\n",
            "Generating train examples...: 8676 examples [00:00, 14221.21 examples/s]\u001b[A\n",
            "Generating train examples...: 10203 examples [00:00, 14556.75 examples/s]\u001b[A\n",
            "Generating train examples...: 11661 examples [00:00, 14107.58 examples/s]\u001b[A\n",
            "Generating train examples...: 13076 examples [00:00, 13758.87 examples/s]\u001b[A\n",
            "Generating train examples...: 14517 examples [00:01, 13951.82 examples/s]\u001b[A\n",
            "Generating train examples...: 15916 examples [00:01, 13814.04 examples/s]\u001b[A\n",
            "Generating train examples...: 17300 examples [00:01, 13673.75 examples/s]\u001b[A\n",
            "Generating train examples...: 18681 examples [00:01, 13713.65 examples/s]\u001b[A\n",
            "Generating train examples...: 20054 examples [00:01, 13636.53 examples/s]\u001b[A\n",
            "Generating train examples...: 21540 examples [00:01, 13996.77 examples/s]\u001b[A\n",
            "Generating train examples...: 23036 examples [00:01, 14282.54 examples/s]\u001b[A\n",
            "Generating train examples...: 24492 examples [00:01, 14364.05 examples/s]\u001b[A\n",
            "Generating train examples...: 25991 examples [00:01, 14550.65 examples/s]\u001b[A\n",
            "Generating train examples...: 27447 examples [00:01, 13852.09 examples/s]\u001b[A\n",
            "Generating train examples...: 28840 examples [00:02, 13839.18 examples/s]\u001b[A\n",
            "Generating train examples...: 30255 examples [00:02, 13927.51 examples/s]\u001b[A\n",
            "Generating train examples...: 31652 examples [00:02, 13885.00 examples/s]\u001b[A\n",
            "Generating train examples...: 33097 examples [00:02, 14049.13 examples/s]\u001b[A\n",
            "Generating train examples...: 34505 examples [00:02, 14048.18 examples/s]\u001b[A\n",
            "Generating train examples...: 35912 examples [00:02, 13821.19 examples/s]\u001b[A\n",
            "Generating train examples...: 37296 examples [00:02, 13638.04 examples/s]\u001b[A\n",
            "Generating train examples...: 38662 examples [00:02, 13590.54 examples/s]\u001b[A\n",
            "Generating train examples...: 40023 examples [00:02, 13376.00 examples/s]\u001b[A\n",
            "Generating train examples...: 41362 examples [00:02, 13030.49 examples/s]\u001b[A\n",
            "Generating train examples...: 42724 examples [00:03, 13198.25 examples/s]\u001b[A\n",
            "Generating train examples...: 44046 examples [00:03, 13053.21 examples/s]\u001b[A\n",
            "Generating train examples...: 45353 examples [00:03, 13053.21 examples/s]\u001b[A\n",
            "Generating train examples...: 46683 examples [00:03, 13125.67 examples/s]\u001b[A\n",
            "                                                                         \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/alpaca/1.0.0.incompleteSXV9YJ/alpaca-train.tfrecord*...:   0% 0/46801 [00:00<?, ? examples/s]\u001b[A\n",
            "Shuffling /root/tensorflow_datasets/alpaca/1.0.0.incompleteSXV9YJ/alpaca-train.tfrecord*...:  99% 46261/46801 [00:00<00:00, 462586.18 examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/alpaca/1.0.0.incompleteSXV9YJ/alpaca-train.tfrecord*. Number of examples: 46801 (shards: [46801])\n",
            "Generating splits...:  50% 1/2 [00:03<00:03,  3.52s/ splits]\n",
            "Generating val examples...: 0 examples [00:00, ? examples/s]\u001b[A\n",
            "Generating val examples...: 1014 examples [00:00, 10135.68 examples/s]\u001b[A\n",
            "Generating val examples...: 2028 examples [00:00, 9802.21 examples/s] \u001b[A\n",
            "Generating val examples...: 3066 examples [00:00, 10058.71 examples/s]\u001b[A\n",
            "Generating val examples...: 4073 examples [00:00, 9779.60 examples/s] \u001b[A\n",
            "Generating val examples...: 5176 examples [00:00, 10216.34 examples/s]\u001b[A\n",
            "                                                                      \u001b[A\n",
            "Shuffling /root/tensorflow_datasets/alpaca/1.0.0.incompleteSXV9YJ/alpaca-val.tfrecord*...:   0% 0/5201 [00:00<?, ? examples/s]\u001b[A\n",
            "INFO[writer.py]: Done writing /root/tensorflow_datasets/alpaca/1.0.0.incompleteSXV9YJ/alpaca-val.tfrecord*. Number of examples: 5201 (shards: [5201])\n",
            "\u001b[1mDataset alpaca downloaded and prepared to /root/tensorflow_datasets/alpaca/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n",
            "INFO[build.py]: Dataset generation complete...\n",
            "\n",
            "tfds.core.DatasetInfo(\n",
            "    name='alpaca',\n",
            "    full_name='alpaca/1.0.0',\n",
            "    description=\"\"\"\n",
            "    TODO(alpaca): Markdown description of that will appear on the catalog page.\n",
            "    Description is **formatted** as markdown.\n",
            "    \n",
            "    It should also contain any processing which has been applied (if any),\n",
            "    (e.g. corrupted example skipped, images cropped,...):\n",
            "    \"\"\",\n",
            "    homepage='https://dataset-homepage/',\n",
            "    data_path='/root/tensorflow_datasets/alpaca/1.0.0',\n",
            "    file_format=tfrecord,\n",
            "    download_size=21.72 MiB,\n",
            "    dataset_size=20.37 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'input': Text(shape=(), dtype=string),\n",
            "        'instruction': Text(shape=(), dtype=string),\n",
            "        'output': Text(shape=(), dtype=string),\n",
            "    }),\n",
            "    supervised_keys=('instruction', 'output'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'train': <SplitInfo num_examples=46801, num_shards=1>,\n",
            "        'val': <SplitInfo num_examples=5201, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"// TODO(alpaca): BibTeX citation\"\"\",\n",
            ")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_ROOT_DIR = \"alpaca\"\n",
        "\n",
        "!mkdir -p {TARGET_TRAIN_DIR}\n",
        "!mkdir -p {TARGET_TEST_DIR}"
      ],
      "metadata": {
        "id": "qeo6meCwVeyi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "builder = tfds.builder(\"alpaca\")\n",
        "builder.download_and_prepare()"
      ],
      "metadata": {
        "id": "X96RYCK-Vnmi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls {builder.data_dir}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVZj_BC5V5Oi",
        "outputId": "78d28f33-454e-4e0f-b30a-95c59d182402"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alpaca-train.tfrecord-00000-of-00001  dataset_info.json\n",
            "alpaca-val.tfrecord-00000-of-00001    features.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp {builder.data_dir}/alpaca-train.tfrecord-00000-of-00001 {TARGET_ROOT_DIR}/alpaca-train.tfrecord\n",
        "!cp {builder.data_dir}/alpaca-val.tfrecord-00000-of-00001 {TARGET_ROOT_DIR}/alpaca-val.tfrecord"
      ],
      "metadata": {
        "id": "SZu1L2-aV7Vp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "3c4F5PMPV9DF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title GCS\n",
        "#@markdown You should change these values as per your preferences. The copy operation can take ~5 minutes.\n",
        "BUCKET_PATH = \"gs://alpaca-tfrecords\" #@param {type:\"string\"}\n",
        "REGION = \"us-central1\" #@param {type:\"string\"}\n",
        "\n",
        "!gsutil mb -l {REGION} {BUCKET_PATH}\n",
        "!gsutil -m cp -r {TARGET_ROOT_DIR}/*.tfrecord {BUCKET_PATH}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9H1HiTpWeU7",
        "outputId": "4a400db0-54ef-46aa-ae9d-8f679a01e2bc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating gs://alpaca-tfrecords/...\n",
            "You are attempting to perform an operation that requires a project id, with none configured. Please re-run gsutil config and make sure to follow the instructions for finding and entering your default project id.\n",
            "Copying file://alpaca/alpaca-train.tfrecord [Content-Type=application/octet-stream]...\n",
            "Copying file://alpaca/alpaca-val.tfrecord [Content-Type=application/octet-stream]...\n",
            "/ [2/2 files][ 21.2 MiB/ 21.2 MiB] 100% Done                                    \n",
            "Operation completed over 2 objects/21.2 MiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XvKW-5PvWybp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}